{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbd97b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from visualizer import TopicVisualizer\n",
    "from preprocessor import IndonesianTextPreprocessor\n",
    "from topic_model import TopicModeler\n",
    "\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctimes)s - %(name)s - %(levelname)s - %(message)s eventId:'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"Current working directory: \", {os.getcwd()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(yaml.dump(config, default_flow_style=False))\n",
    "\n",
    "os.makedirs(config['output']['figures_path'], exist_ok=True)\n",
    "os.makedirs(config['output']['model_path'].rsplit('/', 1)[0], exist_ok=True)\n",
    "os.makedirs(config['data']['processed_path'].rsplit('/', 1)[0], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "    file_path=config['data']['raw_path'],\n",
    "    text_column=config['data']['text_column']\n",
    ")\n",
    "\n",
    "df_raw = loader.load_data()\n",
    "\n",
    "loader.validate_data()\n",
    "\n",
    "stats = loader.get_basic_stats()\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"    {key}:  {value}\")\n",
    "\n",
    "print(\"\\nSample tweets:\")\n",
    "print(loader.sample_data(n=3)[config['data']['text_column']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fab77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f650985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "text_lengths = df_raw[config['data']['text_column']].str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(text_lengths, bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Text Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Tweet Lengths')\n",
    "axes[0].axvline(text_lengths.mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {text_lengths.mean():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].boxplot(text_lengths, vert=True)\n",
    "axes[1].set_ylabel('Text Length (characters)')\n",
    "axes[1].set_title('Tweet Length Box Plot')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(f\"  Mean: {text_lengths.mean():.2f}\")\n",
    "print(f\"  Median: {text_lengths.median():.2f}\")\n",
    "print(f\"  Std: {text_lengths.std():.2f}\")\n",
    "print(f\"  Min: {text_lengths.min()}\")\n",
    "print(f\"  Max: {text_lengths.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = IndonesianTextPreprocessor(\n",
    "    remove_urls=config['preprocessing']['remove_urls'],\n",
    "    remove_mentions=config['preprocessing']['remove_mentions'],\n",
    "    remove_hashtags=config['preprocessing']['remove_hashtags'],\n",
    "    remove_numbers=config['preprocessing']['remove_numbers'],\n",
    "    min_length=config['preprocessing']['min_length'],\n",
    "    max_length=config['preprocessing']['max_length'],\n",
    "    use_stemming=True,  # Important for Indonesian\n",
    "    use_stopword_removal=True  # Remove common words\n",
    ")\n",
    "\n",
    "print(\"✓ Preprocessor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = df_raw[config['data']['text_column']].head(5).tolist()\n",
    "\n",
    "print(\"PREPROCESSING TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    cleaned = preprocessor.clean_text(text)\n",
    "    print(f\"Cleaned: {cleaned}\")\n",
    "    \n",
    "    processed = preprocessor.remove_stopwords_and_stem(cleaned)\n",
    "    print(f\"Processed: {processed}\")\n",
    "    print(f\"Valid: {preprocessor.is_valid_text(processed)}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting full dataset preprocessing...\")\n",
    "print(\"This will take ~5-7 minutes. Please wait...\")\n",
    "print()\n",
    "\n",
    "df_processed = preprocessor.preprocess(\n",
    "    df_raw[config['data']['text_column']].tolist(),\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "df_processed['original_index'] = df_raw.index\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total texts processed: {len(df_processed)}\")\n",
    "print(f\"Valid texts: {df_processed['is_valid'].sum()}\")\n",
    "print(f\"Filtered out: {(~df_processed['is_valid']).sum()}\")\n",
    "print(f\"Filter rate: {(~df_processed['is_valid']).sum() / len(df_processed) * 100:.2f}%\")\n",
    "\n",
    "df_processed.to_csv(config['data']['processed_path'], index=False)\n",
    "print(f\"\\n✓ Processed data saved to {config['data']['processed_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5ee1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "valid_df = df_processed[df_processed['is_valid']]\n",
    "\n",
    "axes[0].scatter(valid_df['original_length'], valid_df['processed_length'], \n",
    "               alpha=0.3, s=10)\n",
    "axes[0].plot([0, valid_df['original_length'].max()], \n",
    "            [0, valid_df['original_length'].max()], \n",
    "            'r--', label='No change line')\n",
    "axes[0].set_xlabel('Original Length')\n",
    "axes[0].set_ylabel('Processed Length')\n",
    "axes[0].set_title('Length Comparison: Original vs Processed')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "length_reduction = (valid_df['original_length'] - valid_df['processed_length']) / valid_df['original_length'] * 100\n",
    "axes[1].hist(length_reduction, bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Length Reduction (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Length Reduction')\n",
    "axes[1].axvline(length_reduction.mean(), color='red', linestyle='--',\n",
    "               label=f'Mean: {length_reduction.mean():.1f}%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average length reduction: {length_reduction.mean():.2f}%\")\n",
    "print(f\"Median length reduction: {length_reduction.median():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62240edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = preprocessor.get_preprocessed_texts(df_processed)\n",
    "\n",
    "print(f\"Total documents for modeling: {len(documents)}\")\n",
    "print(f\"\\nFirst 3 processed documents:\")\n",
    "for i, doc in enumerate(documents[:3], 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332a7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = TopicModeler(\n",
    "    embedding_model=config['model']['embedding_model'],\n",
    "    nr_topics=config['model']['nr_topics'] if config['model']['nr_topics'] != 'auto' else None,\n",
    "    min_topic_size=config['model']['min_topic_size'],\n",
    "    n_gram_range=tuple(config['model']['n_gram_range']),\n",
    "    calculate_probabilities=config['model']['calculate_probabilities']\n",
    ")\n",
    "\n",
    "print(\"✓ Topic model initialized\")\n",
    "print(\"\\nModel configuration:\")\n",
    "print(f\"  Embedding model: {config['model']['embedding_model']}\")\n",
    "print(f\"  Number of topics: {config['model']['nr_topics']}\")\n",
    "print(f\"  Min topic size: {config['model']['min_topic_size']}\")\n",
    "print(f\"  N-gram range: {config['model']['n_gram_range']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1945471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"STARTING TOPIC MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will take approximately 15-20 minutes.\")\n",
    "print(\"Please be patient and don't interrupt the process.\")\n",
    "print()\n",
    "\n",
    "topic_model.train(documents)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee199198",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "print(\"TOPIC OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total topics found: {len(topic_info[topic_info['Topic'] != -1])}\")\n",
    "print(f\"Total documents: {len(documents)}\")\n",
    "\n",
    "outlier_info = topic_info[topic_info['Topic'] == -1]\n",
    "if len(outlier_info) > 0:\n",
    "    outlier_count = outlier_info['Count'].values[0]\n",
    "    print(f\"Outlier documents: {outlier_count} ({outlier_count/len(documents)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nTop 10 Topics:\")\n",
    "print(topic_info[topic_info['Topic'] != -1].head(10)[['Topic', 'Count', 'Name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d795dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_topic(topic_id, n_words=15, n_docs=3):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TOPIC {topic_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    topic_row = topic_info[topic_info['Topic'] == topic_id].iloc[0]\n",
    "    print(f\"Documents: {topic_row['Count']}\")\n",
    "    print(f\"Name: {topic_row['Name']}\")\n",
    "    \n",
    "    print(f\"\\nTop {n_words} words:\")\n",
    "    words = topic_model.get_topic_words(topic_id, top_n=n_words)\n",
    "    for i, (word, score) in enumerate(words, 1):\n",
    "        print(f\"  {i:2d}. {word:20s} (score: {score:.4f})\")\n",
    "    \n",
    "    print(f\"\\nExample documents:\")\n",
    "    doc_topics = topic_model.get_document_topics(documents)\n",
    "    topic_docs = doc_topics[doc_topics['Topic'] == topic_id].head(n_docs)\n",
    "    \n",
    "    for i, (_, row) in enumerate(topic_docs.iterrows(), 1):\n",
    "        print(f\"\\n  Document {i}:\")\n",
    "        print(f\"  {row['Document']}\")\n",
    "        if row['Probability'] is not None:\n",
    "            print(f\"  (Probability: {row['Probability']:.3f})\")\n",
    "\n",
    "for topic_id in topic_info[topic_info['Topic'] != -1]['Topic'].head(3):\n",
    "    examine_topic(topic_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4b328",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = TopicVisualizer(\n",
    "    model=topic_model,\n",
    "    output_dir=config['output']['figures_path']\n",
    ")\n",
    "\n",
    "print(\"✓ Visualizer initialized\")\n",
    "print(f\"Figures will be saved to: {config['output']['figures_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72611829",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = topic_info[topic_info['Topic'] != -1]['Topic'].head(6).tolist()\n",
    "\n",
    "visualizer.plot_multiple_wordclouds(\n",
    "    topic_ids=top_topics,\n",
    "    save=True,\n",
    "    filename=\"wordclouds_top_topics.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee791a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_topic_similarity_heatmap(\n",
    "    top_n=20,\n",
    "    save=True,\n",
    "    filename=\"topic_similarity.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.plot_document_topic_distribution(\n",
    "    documents=documents,\n",
    "    sample_size=1000,\n",
    "    save=True,\n",
    "    filename=\"assignment_confidence.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf5871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_topics = 20\n",
    "# topic_model.reduce_topics(nr_topics=target_topics)\n",
    "# print(f\"Topics reduced to {target_topics}\")\n",
    "\n",
    "# topic_info = topic_model.get_topic_info()\n",
    "# print(topic_info[topic_info['Topic'] != -1].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23005e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.generate_topic_report(\n",
    "    output_file=\"topic_modeling_report.txt\"\n",
    ")\n",
    "\n",
    "print(\"✓ Report generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1328c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save_model(config['output']['model_path'])\n",
    "\n",
    "print(f\"✓ Model saved to {config['output']['model_path']}\")\n",
    "print(\"\\nModel files:\")\n",
    "print(f\"  - {config['output']['model_path']}\")\n",
    "print(f\"  - {config['output']['model_path'].replace('.pkl', '_embeddings.pkl')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4c9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'document': documents,\n",
    "    'topic': topic_model.topics,\n",
    "    'probability': topic_model.probs if topic_model.probs is not None else [None] * len(documents)\n",
    "})\n",
    "\n",
    "results_df = results_df.merge(\n",
    "    df_processed[df_processed['is_valid']][['original_text', 'original_index']],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "topic_names = topic_info.set_index('Topic')['Name'].to_dict()\n",
    "results_df['topic_name'] = results_df['topic'].map(topic_names)\n",
    "\n",
    "output_path = config['output']['results_path'] + 'topic_assignments.csv'\n",
    "results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Results exported to {output_path}\")\n",
    "print(f\"\\nExported {len(results_df)} document-topic assignments\")\n",
    "print(\"\\nSample results:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99179696",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TOPIC MODELING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Raw documents: {len(df_raw)}\")\n",
    "print(f\"  After preprocessing: {len(documents)}\")\n",
    "print(f\"  Filter rate: {(1 - len(documents)/len(df_raw))*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTopics:\")\n",
    "n_topics = len(topic_info[topic_info['Topic'] != -1])\n",
    "print(f\"  Total topics: {n_topics}\")\n",
    "print(f\"  Average documents per topic: {len(documents)/n_topics:.0f}\")\n",
    "\n",
    "if -1 in topic_info['Topic'].values:\n",
    "    outliers = topic_info[topic_info['Topic'] == -1]['Count'].values[0]\n",
    "    print(f\"  Outliers: {outliers} ({outliers/len(documents)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTop 5 topics:\")\n",
    "for _, row in topic_info[topic_info['Topic'] != -1].head(5).iterrows():\n",
    "    print(f\"  Topic {row['Topic']}: {row['Count']} docs - {row['Name'][:60]}\")\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Embedding model: {config['model']['embedding_model']}\")\n",
    "print(f\"  Min topic size: {config['model']['min_topic_size']}\")\n",
    "\n",
    "print(f\"\\nOutputs:\")\n",
    "print(f\"  Model: {config['output']['model_path']}\")\n",
    "print(f\"  Figures: {config['output']['figures_path']}\")\n",
    "print(f\"  Results: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ TOPIC MODELING COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b66b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
